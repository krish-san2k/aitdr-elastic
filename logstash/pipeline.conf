input {
  kafka {
    bootstrap_servers => "kafka:9092"
    topics => ["events"]
    codec => json
  }
}

filter {
  # parse common fields - customize with grok
  if [message] {
    mutate { add_field => { "raw_message" => "%{message}" } }
  }
  
  # example extract src/dst ip for network events
  if [source_ip] and [dest_ip] {
    # IPs already present
  } else if [message] {
    grok {
      match => { "message" => "%{IP:source_ip} %{IP:dest_ip}" }
      tag_on_failure => ["_grokparsefailure"]
    }
  }
  
  # Normalize timestamp
  if [timestamp] {
    date { 
      match => ["timestamp", "ISO8601", "yyyy-MM-dd HH:mm:ss"]
      target => "@timestamp"
    }
  }
  
  # Add processing metadata
  mutate {
    add_field => { "logstash_timestamp" => "%{@timestamp}" }
    add_field => { "processed_by" => "logstash" }
  }
  
  # Example enrichment - add severity if not present
  if ![severity] {
    mutate { add_field => { "severity" => 5.0 } }
  }
}

output {
  # Send to Elasticsearch
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "raw-logs-%{+YYYY.MM.dd}"
    action => "index"
  }
  
  # Optional: Send to console for debugging
  stdout {
    codec => json_lines
  }
}
